' For what it\'s worth.',
' gmafb',
' give me a fucking break',
' imo',
' in my humble opinion',
' jk',
' just kidding',
' irl',
' in real life',
' iirc',
' if I remember correctly',
' lmk',
' let me know',
' lol',
' laugh out loud',
' nbd',
' no big deal',
' nct',
' nobody cares though',
' nfw',
' no fucking way',
' njoy',
' enjoy',
' omfg',
' oh my fucking god',
' plmk',
' please let me know',
' rtfm',
' read the fucking manual',
' sob',
' son of a bitch',
' srs',
' serious',
' orly',
' oh really',
' stfu',
' shut the fuck up',
' wtv',
' whatever',
' yw',
' you are welcome',
' u',
' you',
' ya',
' you',
' prt',
' please retweet',
' r',
' are',
' b4',
' before',
' ic',
' i see',
' some1',
' someone',
' yrs',
' years',
' hrs',
' hours',
' mins',
' minutes',
' secs',
' seconds',
' pls',
' please',
' plz',
' please',
' 2morow',
' tomorrow',
' 2day',
' today',
' lil',
' little',
' ill',
' i will',
' i\'ll',
' i will',
' im',
' i am',
' ama',
' i am',
" i\'m",
' i am',
' ive',
' i have',
' iva',
' i have',
" i've",
' i have',
' 4ever',
' for ever',
' 4got',
' forget',
' 4gotten',
' forget',
" you\'re",
" you are",
" he\'s",
" hes is",
" she\'s",
" she is",
" we\'re",
" we are",
" they\'re",
" they are",
" isn't",
" is not",
" aren't",
" are not",
" hasn't",
" has not",
" haven't",
" have not",
" hadn't",
" had not",
" won't",
" will not",
" wouldn't",
" would not",
" shouldn't",
" should not")
df_abr <- tibble(abreviation=abr[seq(from=1, to=length(abr), by=2)], word=abr[seq(from =2, to=length(abr), by=2)])
neg <- c(" can't",
" can not",
" couldn't",
" could not",
" didn't",
" did not",
" doesn't",
" does not",
" wasn't",
" was not",
" wouldn't",
" would not",
" don't",
" do not",
" weren't",
" were not")
df_neg <- tibble(abreviation=neg[seq(from=1, to=length(neg), by=2)], word=neg[seq(from =2, to=length(neg), by=2)])
df_abr <- rbind(df_abr, df_neg)
stop <- tm::stopwords()
abr <- str_split(str_trim(df_abr$word), ' ') %>%
unlist
stop <- stop[!stop %in% c(abr, "be", "been", "being", "having", "doing")]
df_clean <- df_tweet
for(i in 1: nrow(df_abr)){
df_clean$tweet <-  str_replace_all(df_clean$tweet, df_abr$abreviation[i], df_abr$word[i])
}
df_clean2 <- df_clean %>%
mutate(tweet=cleaning(tweet))
df_clean3 <- df_clean2 %>%
filter(stringi::stri_count_words(tweet)>3)  %>%
mutate(tweet=str_replace_all(tweet,'[^A-Za-z]', ' ') )
df_words <- df_clean3 %>%
unnest_tokens(word, tweet) %>%
mutate(lemmatize=lemmatize_words(word)) #%>%
# select(word, lemmatize) %>%
# distinct()
df_clean4 <- df_words %>%
group_by(id, positive) %>%
summarise(text= paste(lemmatize, collapse=' '))
library(keras)
texts <- df_clean4$text
labels <- df_clean4$positive
maxlen <- 250
training_samples <- floor(nrow(df_clean4)*0.8)
validation_samples <- floor(nrow(df_clean4)*0.2)
max_words <- 50000
tokenizer <- text_tokenizer(num_words = max_words) %>%
fit_text_tokenizer(texts)
sequences <- texts_to_sequences(tokenizer, texts)
word_index <- tokenizer$word_index
mydata <- pad_sequences(sequences, maxlen = maxlen)
ind = 1:nrow(df_clean4)
training_ind <- sample(ind, training_samples, replace=F)
test_ind <- ind[!ind %in% c(training_ind)]
x_train <- mydata[training_ind,]
y_train <- labels[training_ind]
x_test <- mydata[test_ind,]
y_test <- labels[test_ind]
model <- keras_model_sequential() %>%
layer_embedding(input_dim = max_words, output_dim = 32) %>%
layer_lstm(units= 32) %>%
layer_dense(units = 1, activation = 'sigmoid')
model %>%
compile(
optimizer = 'rmsprop',
loss = 'binary_crossentropy',
metrics = c('acc')
)
summary(model)
history <- model %>%
fit(x_train, y_train, epoch = 10, batch_size=128, validation_split = 0.25)
save_model_hdf5(model, 'model/tweet.h5')
model %>%
evaluate(x_test, y_test)
library(dplyr)
library(tidytext)
library(openxlsx)
library(stringr)
library(purrr)
library(lubridate)
library(highcharter)
library('widyr')
path_tweet = '/Users/woza/Documents/dataScience/bdl_presentation/data/tidytext/'
files <- list.files(path = path_tweet)
cv_files <- files[str_detect(files,'.csv')]
xlsx_files <- files[str_detect(files,'.xlsx')]
tweet <- paste0(path_tweet,xlsx_files) %>%
map_dfr(read.xlsx) %>%
mutate(Date = mdy(str_sub(Date,1,10)))
csv_tweet <- read.csv(paste0(path_tweet, cv_files))%>%
mutate(Date = ymd(str_sub(Date,1,10)))
tweet <- rbind(tweet[,1:3], csv_tweet[,1:3])
tweet2 <- tweet %>%
distinct() %>%
rename(message=Tweet) %>%
mutate(date_time =  ymd_hms(paste(Date, Time)),
message_punctuationless= str_replace_all(message, '[[^@\\w\\s]]', ' '),
user=str_extract_all(message_punctuationless, '@(\\w){1,15}'),
#is_reply = ifelse( str_detect(message, '"@'),1,0),
#reply_user = ifelse(is_reply, str_extract(message, '@(\\w){1,15}'), NA),
is_rt = ifelse( str_detect(message, '^ RT @|^RT @|"@'),1,0),
rt_user = ifelse(is_rt, str_extract(message, '@(\\w){1,15}'), NA),
message2 = str_remove_all(message, '(http|https):\\/\\/([\\w\\d\\.\\:]*)(\\/?)(.*)'),
text = ifelse(is_rt & ((str_detect(message2,'^"') & str_count(message2,'"')==1|str_detect(message2,'^RT'))),'',
ifelse(is_rt , str_remove_all(message2, '".*"'), message2)),
hashtag = str_extract_all(message, '#(\\w){1,30}'))  %>% #just the 1st user, the original author
select(-message_punctuationless, -message2)
df_tweet <- tweet2
for(i in 1: nrow(df_abr)){
df_tweet$text <-  str_replace_all(df_tweet$text, df_abr$abreviation[i], df_abr$word[i])
}
df_tweet2 <- df_tweet %>%
mutate(text=cleaning(text))
df_tweet3 <- df_tweet2 %>%
filter(stringi::stri_count_words(text)>3)  %>%
mutate(text=str_replace_all(text,'[^A-Za-z]', ' ') )
View(df_tweet3)
x_test[1]
x_test[1:3]
x_train[1:3]
x_train[1:3,]
model %>%
predict(df_tweet3$text)
word_index[1:3,]
word_index[1:3]
new_text <- texts_to_sequences(tokenizer, df_tweet3)
new_text <- texts_to_sequences(tokenizer, df_tweet3$text)
pred <- model %>%
predict(new_text)
new_text <- texts_to_sequences(tokenizer, df_tweet3$text)
new_text <-  pad_sequences(new_text, maxlen = maxlen)
new_text
pred <- model %>%
predict(new_text)
df_tweet4 <- cbind(df_tweet3, pred)
getwd()
write_rds(df_tweet4, '/Users/woza/Documents/dataScience/bdl_presentation/data/tidytext/tweet_pred')
getwd()
save_model_hdf5(model, '/Users/woza/Documents/dataScience/sentiment_analysis/model/tweet.h5')
save_text_tokenizer(tokenizer, 'model/tokenizer')
save_text_tokenizer(tokenizer, '/Users/woza/Documents/dataScience/sentiment_analysis/model/tokenizer')
library(dplyr)
library(tidytext)
library(openxlsx)
library(stringr)
library(purrr)
library(lubridate)
library(highcharter)
library(widyr)
library(reactable)
library(DT)
path_tweet = '/Users/woza/Documents/dataScience/bdl_presentation/data/tidytext/'
files <- list.files(path = path_tweet)
cv_files <- files[str_detect(files,'.csv')]
xlsx_files <- files[str_detect(files,'.xlsx')]
tweet <- read_rds(paste0(path_tweet, 'tweet_trump_sentiment.rds'))
library(readr)
tweet <- read_rds(paste0(path_tweet, 'tweet_trump_sentiment.rds'))
View(tweet)
tweet_fox <- tweet %>%
filter(str_detect(message, '@FoxNews')) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
mutate(word=case_when(
word %in% c('barackobama\'s', 'obama\'s', 'barackobama', 'obama.i') ~ 'obama',
word %in% c('interviewed') ~ 'interview',
word %in% c('chris', 'wallace') ~  'chriswallace',
word=='jobs' ~ 'job',
TRUE ~ as.character(word))) %>%
group_by(word) %>%
summarise(count=n()) %>%
arrange(desc(count)) %>%
filter(!word %in% c('trump', 'p.m', 'a.m', '10', '00', 'foxnewssunday',
'fox', 'foxnews')) %>%
slice(1:15) %>%
mutate(rank= row_number(),
word=factor(word, level=.$word[rank]),
count2=count,
count=-count)
View(tweet_fox)
View(tweet)
tweet_fox <- tweet %>%
filter(str_detect(message, '@FoxNews')) %>%
summarize(count=n, positive_nlp = sum(is_positive_nlp), positive_word = sum(is_positive_word))
tweet_fox <- tweet %>%
filter(str_detect(message, '@FoxNews')) %>%
summarize(count=n(), positive_nlp = sum(is_positive_nlp), positive_word = sum(is_positive_word))
View(tweet_fox)
tweet_fox <- tweet %>%
filter(str_detect(message, '@FoxNews')) %>%
summarize(count=n(), positive_nlp = sum(is_positive_nlp, na.rm = TRUE), positive_word = sum(is_positive_word, na.rm = TRUE))
tweet_cnn <- tweet %>%
filter(str_detect(message, '@CNN')) %>%
summarize(count=n(), positive_nlp = sum(is_positive_nlp, na.rm = TRUE), positive_word = sum(is_positive_word, na.rm = TRUE))
View(tweet_cnn)
View(tweet_fox)
tweet_fox_cnn <- tweet %>%
filter(str_detect(message, '@FoxNews') & str_detect(message, '@CNN'))
View(tweet_fox_cnn)
tweet_cnn <- tweet %>%
filter(str_detect(message, '@CNN'))
View(tweet_cnn)
tweet_channel <- rbind(tweet_cnn, tweet_fox )
tweet_fox <- tweet %>%
filter(str_detect(message, '@FoxNews')) %>%
summarize(count=n(), positive_nlp = sum(is_positive_nlp, na.rm = TRUE), positive_word = sum(is_positive_word, na.rm = TRUE)) %>%
mutate(channel='FoxNews')
tweet_cnn <- tweet %>%
filter(str_detect(message, '@CNN')) %>%
summarize(count=n(), positive_nlp = sum(is_positive_nlp, na.rm = TRUE), positive_word = sum(is_positive_word, na.rm = TRUE)) %>%
mutate(channel='CNN')
tweet_channel <- rbind(tweet_cnn, tweet_fox )
View(tweet_channel)
View(tweet)
tweet_fox <- tweet %>%
filter(str_detect(message, '@FoxNews')  & str_count(text_trump)>0) %>%
summarize(count=n(), positive_nlp = sum(is_positive_nlp, na.rm = TRUE), positive_word = sum(is_positive_word, na.rm = TRUE)) %>%
mutate(channel='FoxNews')
tweet_cnn <- tweet %>%
filter(str_detect(message, '@CNN') & str_count(text_trump)>0) %>%
summarize(count=n(), positive_nlp = sum(is_positive_nlp, na.rm = TRUE), positive_word = sum(is_positive_word, na.rm = TRUE)) %>%
mutate(channel='CNN')
tweet_channel <- rbind(tweet_cnn, tweet_fox )
tweet_channel <- rbind(tweet_cnn, tweet_fox ) %>%
mutate(pct = positive_nlp/count)
hchart(tweet_channel,'column', hcaes(y=pct, group=channel),
tooltip = list(pointFormat = paste0("Nb tweets: <b>{point.count}</b>")))
tweet_channel <- rbind(tweet_cnn, tweet_fox ) %>%
mutate(pct = round(positive_nlp/count,2)*100)
hchart(tweet_channel,'column', hcaes(y=pct, group=channel) color='#de6263',
tooltip = list(pointFormat = paste0("Nb tweets: <b>{point.count}</b>")))
hchart(tweet_channel,'column', hcaes(y=pct, group=channel), color='#de6263',
tooltip = list(pointFormat = paste0("Nb tweets: <b>{point.count}</b>")))
hchart(tweet_channel,'column', hcaes(y=pct, group=channel), color=c('#7bb4ec','#de6263'),
tooltip = list(pointFormat = paste0("Nb tweets: <b>{point.count}</b>")))
hchart(tweet_channel,'column', hcaes(y=pct, group=channel), color=c('#7bb4ec','#de6263'),
tooltip = list(pointFormat = paste0("Nbre tweets: <b>{point.count}</b></br> ")))
hchart(tweet_channel,'column', hcaes(y=pct, fill=channel), color=c('#7bb4ec','#de6263'),
tooltip = list(pointFormat = paste0("Nbre tweets: <b>{point.count}</b></br> ")))
hchart(tweet_channel,'column', hcaes(y=pct, group=channel), color=c('#7bb4ec','#de6263'),
tooltip = list(pointFormat = paste0("Nbre tweets: <b>{point.count}</b></br>Nbre tweets positifs: <b>{point.positive_nlp}</b> ")))
hchart(tweet_channel,'column', hcaes(y=pct, group=channel), color=c('#7bb4ec','#de6263'),
tooltip = list(pointFormat = paste0("Nbre tweets: <b>{point.count}</b></br>Nbre tweets positifs: <b>{point.positive_nlp}</b> "))) %>%
hc_yAxis(title = list(text = 'pourcentage (%)')) %>%
hc_title(text='Tweets sont plus positifs quand @FoxNews est mentionné')
hchart(tweet_channel,'column', hcaes(y=pct, group=channel), color=c('#7bb4ec','#de6263'),
tooltip = list(pointFormat = paste0("Nbre tweets: <b>{point.count}</b></br>Nbre tweets positifs: <b>{point.positive_nlp}</b> "))) %>%
hc_yAxis(title = list(text = 'pourcentage (%)')) %>%
hc_title(text='Tweets plus positifs quand<br>@FoxNews est mentionné')
hchart(tweet_channel,'column', hcaes(y=pct, group=channel), color=c('#7bb4ec','#de6263'),
tooltip = list(pointFormat = paste0("Nbre tweets: <b>{point.count}</b></br>Nbre tweets positifs: <b>{point.positive_nlp}</b> "))) %>%
hc_yAxis(title = list(text = 'pourcentage (%)')) %>%
hc_xAxis(type = "datetime", labels = list(enabled = FALSE)) %>%
hc_title(text='Tweets plus positifs quand<br>@FoxNews est mentionné')
tweet_fox <- tweet %>%
filter(str_detect(message, '@FoxNews')  & str_count(text_trump)>0) %>%
summarize(count=n(), positive_nlp = sum(is_positive_nlp, na.rm = TRUE), positive_word = sum(is_positive_word, na.rm = TRUE)) %>%
mutate(channel='FoxNews')
tweet_cnn <- tweet %>%
filter(str_detect(message, '@CNN') & str_count(text_trump)>0) %>%
summarize(count=n(), positive_nlp = sum(is_positive_nlp, na.rm = TRUE), positive_word = sum(is_positive_word, na.rm = TRUE)) %>%
mutate(channel='CNN')
tweet_channel <- rbind(tweet_fox,tweet_cnn ) %>%
mutate(pct = round(positive_nlp/count,2)*100)
View(tweet_channel)
tweet_fox <- tweet %>%
filter(str_detect(message, '@FoxNews')  & str_count(text_trump)>0) %>%
summarize(count=n(), positive_nlp = sum(is_positive_nlp, na.rm = TRUE), positive_word = sum(is_positive_word, na.rm = TRUE)) %>%
mutate(channel='FoxNews')
tweet_cnn <- tweet %>%
filter(str_detect(message, '@CNN') & str_count(text_trump)>0) %>%
summarize(count=n(), positive_nlp = sum(is_positive_nlp, na.rm = TRUE), positive_word = sum(is_positive_word, na.rm = TRUE)) %>%
mutate(channel='CNN')
tweet_channel <- rbind(tweet_fox,tweet_cnn ) %>%
mutate(pct = round(positive_nlp/count,2)*100)
hchart(tweet_channel,'column', hcaes(y=pct, group=channel), color=c('#de6263', '#7bb4ec'),
tooltip = list(pointFormat = paste0("Nbre tweets: <b>{point.count}</b></br>Nbre tweets positifs: <b>{point.positive_nlp}</b> "))) %>%
hc_yAxis(title = list(text = 'pourcentage (%)')) %>%
hc_xAxis(type = "datetime", labels = list(enabled = FALSE)) %>%
hc_title(text='Tweets plus positifs quand<br>@FoxNews est mentionné')
tweet_channel <- rbind(tweet_fox,tweet_cnn ) %>%
mutate(pct = round(positive_nlp/count,2)*100, order = n())
library(dplyr)
library(tidytext)
library(readr)
library(stringr)
library(purrr)
library(lubridate)
library(highcharter)
library(widyr)
library(reactable)
library(DT)
path_tweet = '/Users/woza/Documents/dataScience/bdl_presentation/data/tidytext/'
tweet <- read_rds(paste0(path_tweet, 'tweet_trump_sentiment.rds'))
nb_tweet <- tweet %>%
mutate(month = lubridate::month(Date, label=TRUE),
year = year(Date),
quarter = lubridate::quarter(Date, with_year = TRUE)) %>%
group_by(month, year) %>%
mutate(count=n())%>%
arrange(Date) %>%
slice(1) %>%
mutate(Date= ymd(paste0(str_sub(Date,1,8),'01'))) %>%
ungroup() %>%
arrange(Date)
hchart(nb_tweet,'line', hcaes(Date,count),
tooltip = list(pointFormat = paste0("Nb tweets: <b>{point.count}</b>"))) %>%
hc_xAxis(type = "datetime", dateTimeLabelFormats = list(day = '%M')) %>%
hc_yAxis(title = list(text = '#Tweets')) %>%
hc_title(text='Grandement Actif sur Twitter') %>%
hc_plotOptions(line = list(
marker = list(
enabled = FALSE
)))
#reactable(head(tweet,2))
datatable(head(tweet %>% select(Date, Time, message),5),  rownames = FALSE, options = list(pageLength = 5, dom = 't') )
datatable(head(tweet %>%  select(Date, Time, message, text_trump, hashtag, is_rt, rt_user, is_positive_word, is_positive_nlp) %>%  rename(`is RT`=is_rt, `user RT`=rt_user, text = text_trump, `positive by words`=is_positive_word, `positive by nlp` =is_positive_nlp),50),  rownames = FALSE, width="100%",options = list(pageLength = 5, dom = 't') )
df_top_user <- tibble(user = unlist(tweet$user)) %>%
group_by(user) %>%
summarise(count = n()) %>%
arrange(desc(count)) %>%
slice(2:11)
hchart(df_top_user, 'bar', hcaes(user, count),
tooltip = list(pointFormat = paste0("Mentionné: <b>{point.count}</b> fois"))) %>%
hc_xAxis(title = list(text ="User")) %>%
hc_yAxis(title = list(text = '#Mentions')) %>%
hc_title(text='Son meilleur ami, Barack Obama')
tweet_cnn <- tweet %>%
filter(str_detect(message, '@CNN')) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
mutate(word=case_when(
word %in% c('barackobama\'s', 'obama\'s', 'barackobama', 'obama.i') ~ 'obama',
word %in% c('interviewed') ~ 'interview',
word %in% c('chris', 'wallace') ~  'chriswallace',
word=='jobs' ~ 'job',
TRUE ~ as.character(word))) %>%
group_by(word) %>%
summarise(count=n()) %>%
arrange(desc(count)) %>%
filter(!word %in% c('trump', 'p.m', 'a.m', '10', '00', '9','cnn')) %>%
slice(1:15) %>%
mutate(rank= row_number(),
word=factor(word, level=.$word[rank]))
tweet_fox <- tweet %>%
filter(str_detect(message, '@FoxNews')) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
mutate(word=case_when(
word %in% c('barackobama\'s', 'obama\'s', 'barackobama', 'obama.i') ~ 'obama',
word %in% c('interviewed') ~ 'interview',
word %in% c('chris', 'wallace') ~  'chriswallace',
word=='jobs' ~ 'job',
TRUE ~ as.character(word))) %>%
group_by(word) %>%
summarise(count=n()) %>%
arrange(desc(count)) %>%
filter(!word %in% c('trump', 'p.m', 'a.m', '10', '00', 'foxnewssunday',
'fox', 'foxnews')) %>%
slice(1:15) %>%
mutate(rank= row_number(),
word=factor(word, level=.$word[rank]),
count2=count,
count=-count)
c1 <- hchart(tweet_cnn, 'bar', hcaes(word, count, fill=word),
name='Occurence',  tooltip = list(pointFormat = paste0("Mentionné: <b>{point.count}</b> fois"))) %>% hc_xAxis(opposite=TRUE) %>% hc_yAxis(title = list(
text= ' '),
labels = list(
formatter = JS("function () {
return Math.abs(this.value) ;
}")), max = 70)
c2 <- hchart(tweet_fox, 'bar', hcaes(word, count, fill=word), color='#de6263',
name='Occurence',  tooltip = list( pointFormat = paste0("Mentionné: <b>{point.count2}</b> fois"))) %>% hc_yAxis(title = list(
text= ' '),
labels = list(
formatter = JS("function () {
return Math.abs(this.value) ;
}")))
lst <- list(
c2,
c1
)
hw_grid(lst, ncol = 2)
View(tweet_channel)
afinn <- get_sentiments("afinn")
score_bywords <- tweet %>%
unnest_tokens(word, text) %>%
left_join(afinn, by='word')
View(score_bywords)
View(tweet)
